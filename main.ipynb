{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "00vXXUJtXpXw",
        "SS7nHgEndZa8",
        "mD0Xmk_ozRlL",
        "aJ_l0GEqUFte"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Article generator"
      ],
      "metadata": {
        "id": "N3Hf51xTeSiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting Required Inputs\n",
        "\n",
        "* Topic on which article has to be generated\n",
        "* No of websites to be reffered to"
      ],
      "metadata": {
        "id": "00vXXUJtXpXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_term = str(input(\"Please enter the topic on which article has to be generated-\"))\n",
        "print(search_term)"
      ],
      "metadata": {
        "id": "HIEfRqWpg00H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7713f508-db6b-4bf0-cfdd-824543fc57bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the topic on which article has to be generated-Random forest algorithm\n",
            "Random forest algorithm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    try:\n",
        "        no_of_urls = int(input(\"Enter the no of websites to visit. (Max:10) \"))\n",
        "        assert 0 < no_of_urls <= 10\n",
        "    except ValueError:\n",
        "        print(\"Please enter a valid number!\")\n",
        "    except AssertionError:\n",
        "        print(\"Please enter a number between 1 and 10!\")\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "id": "i-pdguvA2OAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de982e65-21cd-4778-9778-439be4789c9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the no of websites to visit. (Max:10) 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bing Search API\n",
        "\n",
        "* Fetch the urls relevant to the search query\n",
        "* Only return webpages as result (no news, image, or video links)\n",
        "* Fetch **n** no of results based on user input"
      ],
      "metadata": {
        "id": "SS7nHgEndZa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure settings\n",
        "subscription_key = \"a018086497784da9a60874870fb6fad2\"\n",
        "assert subscription_key\n",
        "search_url = \"https://api.bing.microsoft.com/v7.0/search\""
      ],
      "metadata": {
        "id": "J9UCsk7Eg0vs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch Data\n",
        "import requests\n",
        "headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
        "params = {\"q\": search_term, \"responseFilter\": \"webpages\", \"count\": no_of_urls}\n",
        "response = requests.get(search_url, headers=headers, params=params)\n",
        "response.raise_for_status()\n",
        "search_results = response.json()"
      ],
      "metadata": {
        "id": "8gtCFICQg0rI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store all urls in a list\n",
        "results = search_results['webPages']['value']\n",
        "url_list = []\n",
        "for result in results:\n",
        "  url_list.append(result[\"url\"])\n",
        "print(url_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp2wvn_pzhdN",
        "outputId": "cc216df8-1274-4f39-8af0-62576bda7842"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://towardsdatascience.com/random-forest-overview-746e7983316', 'https://www.ibm.com/tw-en/topics/random-forest', 'https://journals.sagepub.com/doi/pdf/10.1177/1536867X20909688', 'https://stackoverflow.com/questions/31344732/a-simple-explanation-of-random-forest', 'https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a', 'https://www.statology.org/random-forest-in-r/', 'https://emeritus.org/in/learn/data-science-random-forest/', 'https://medium.com/mlearning-ai/an-exploration-into-tensorflows-random-forest-algorithm-f9be91f1850b', 'https://www.mdpi.com/2227-7390/11/7/1636', 'https://pubmed.ncbi.nlm.nih.gov/21309739/']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview of collected websites\n",
        "from IPython.display import HTML\n",
        "\n",
        "rows = \"\\n\".join([\"\"\"<tr>\n",
        "                       <td><a href=\\\"{0}\\\">{1}</a></td>\n",
        "                       <td>{2}</td>\n",
        "                     </tr>\"\"\".format(v[\"url\"], v[\"name\"], v[\"snippet\"])\n",
        "                  for v in search_results[\"webPages\"][\"value\"]])\n",
        "HTML(\"<table>{0}</table>\".format(rows))"
      ],
      "metadata": {
        "id": "mvZSgMfx8r4-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7009640d-f18e-44ef-eb8c-768af20e2866"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table><tr>\n",
              "                       <td><a href=\"https://towardsdatascience.com/random-forest-overview-746e7983316\">Random Forest Overview. A conceptual overview of the Random… | by ...</a></td>\n",
              "                       <td>The Random forest is an ensemble method (it groups multiple Decision tree predictors) which was developed by Leo Breiman in 2001². In [2] Breiman states that ‘The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them’.</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://www.ibm.com/tw-en/topics/random-forest\">What is Random Forest? | IBM</a></td>\n",
              "                       <td>Random forest is a commonly-used machine learning algorithm trademarked by Leo Breiman and Adele Cutler, which combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. Decision trees</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://journals.sagepub.com/doi/pdf/10.1177/1536867X20909688\">The random forest algorithm for statistical learning - SAGE Journals</a></td>\n",
              "                       <td>4 The random forest algorithm for statistical learning Random forest is one of the best-performing learning algorithms. For social scien-tists, such developments in algorithms are useful only to the extent that they can access an implementation of the algorithm. In this article, we introduce rforest, a command</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://stackoverflow.com/questions/31344732/a-simple-explanation-of-random-forest\">algorithm - A simple explanation of Random Forest - Stack Overflow</a></td>\n",
              "                       <td>A random forest is a collection of random decision trees (of number n_estimators in sklearn). What you need to understand is how to build one random decision tree. Roughly speaking, to build a random decision tree you start from a subset of your training samples. At each node you will draw randomly a subset of features (number determined by max ...</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a\">Master Machine Learning: Random Forest From Scratch With Python</a></td>\n",
              "                       <td>The random forest algorithm is based on the bagging method. It represents a concept of combining learning models to increase performance (higher accuracy or some other metric). In a nutshell: N subsets are made from the original datasets; N decision trees are build from the subsets;</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://www.statology.org/random-forest-in-r/\">How to Build Random Forests in R (Step-by-Step) - Statology</a></td>\n",
              "                       <td>It turns out that random forests tend to produce much more accurate models compared to single decision trees and even bagged models. This tutorial provides a step-by-step example of how to build a random forest model for a dataset in R. Step 1: Load the Necessary Packages. First, we’ll load the necessary packages for this example.</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://emeritus.org/in/learn/data-science-random-forest/\">What is Random Forest In Data Science and How Does it Work?</a></td>\n",
              "                       <td>Finally, random forest algorithms can be used for time series forecasting tasks such as predicting stock prices or sales volumes over time. The Cons of Random Forest Data Science. This predictive model has been widely adopted due to its ability to handle large datasets and its ability to accurately predict complex relationships between ...</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://medium.com/mlearning-ai/an-exploration-into-tensorflows-random-forest-algorithm-f9be91f1850b\">An exploration into Tensorflow’s Random Forest algorithm</a></td>\n",
              "                       <td>Random forest is an ensemble learning method that combines multiple decision trees to create a more accurate and robust model. It works by creating a large number of decision trees, each...</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://www.mdpi.com/2227-7390/11/7/1636\">Random Forest and Whale Optimization Algorithm to Predict the ...</a></td>\n",
              "                       <td>The problem of backfilling pipeline invalidation has become a bottleneck restricting the application and development of backfilling technology. This study applied the whale optimization algorithm and random forest (WOA–RF) to predict the invalidation risk of backfilling pipelines based on 59 datasets from actual mines. Eight influencing factors of backfilling pipeline invalidation risk were ...</td>\n",
              "                     </tr>\n",
              "<tr>\n",
              "                       <td><a href=\"https://pubmed.ncbi.nlm.nih.gov/21309739/\">Using random forest algorithm to predict β-hairpin motifs</a></td>\n",
              "                       <td>That is Random Forest algorithm on the basis of the multi-characteristic parameters, which include amino acids component of position, hydropathy component of position, predicted secondary structure information and value of auto-correlation function. Firstly, the method is trained and tested on a set of 8,291 β-hairpin motifs and 6,865 non-β ...</td>\n",
              "                     </tr></table>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Web Scraping\n",
        "* Obtain relevant information\n",
        "* Seperate functions for some common data sources \n",
        "    * Wikipedia\n",
        "    * GeeksforGeeks\n",
        "    * Towards Data Science\n",
        "    * W3schools\n",
        "    * Analyticsvidhya\n",
        "    * Tutorials point\n",
        "    * JavaTpoint"
      ],
      "metadata": {
        "id": "mD0Xmk_ozRlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia"
      ],
      "metadata": {
        "id": "6HPonsBZ8y86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk4KRCuDFyGP",
        "outputId": "365827b4-41cb-4685-ef2d-869125d45343"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from wikipedia-api) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->wikipedia-api) (2.0.12)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Wikipedia API\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipediaapi\n",
        "\n",
        "def wikipedia(wiki_url):\n",
        "      page_name = wiki_url.split(\"/\")[-1]\n",
        "\n",
        "      wikipedia_obj = wikipediaapi.Wikipedia(\n",
        "        language='en',\n",
        "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
        "      )\n",
        "\n",
        "      page = wikipedia_obj.page(page_name)\n",
        "\n",
        "      if(page.exists()):\n",
        "          return page.text\n",
        "\n",
        "      return \" \"\n"
      ],
      "metadata": {
        "id": "olZJfpHWDd95"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wikipedia(\"https://en.wikipedia.org/wiki/Random_forest\")"
      ],
      "metadata": {
        "id": "W-oSKPRoDMEa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GeeksForGeeks"
      ],
      "metadata": {
        "id": "sc9LgH2F9HHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def geeksforgeeks(gfg_url):\n",
        "    # Make a GET request to the topic URL\n",
        "    response = requests.get(gfg_url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content of the topic page (excluding code snippets)\n",
        "    content = ''\n",
        "    for tag in soup.find_all('div',{'class':'text'}):\n",
        "        content += tag.text.strip() \n",
        "\n",
        "    return content\n"
      ],
      "metadata": {
        "id": "IIstW9t43q_t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# geeksforgeeks(\"https://www.geeksforgeeks.org/random-forest-regression-in-python/\")"
      ],
      "metadata": {
        "id": "lIraRfDX5R93"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### W3Schools"
      ],
      "metadata": {
        "id": "X_hpB1we9Npo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def w3schools(url):\n",
        "    # Make a GET request to the topic URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content of the topic page (excluding code snippets)\n",
        "    content = ''\n",
        "    for tag in soup.find_all('div',{'id':'main'}):\n",
        "        for sub_tag in tag.find_all('p'):\n",
        "          content += sub_tag.text.strip() \n",
        "\n",
        "    return content\n"
      ],
      "metadata": {
        "id": "PYsR1lKDJOk7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# w3schools(\"https://www.w3schools.com/python/python_ml_decision_tree.asp\")"
      ],
      "metadata": {
        "id": "8EB6xkMMILE1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TowardsDataScience"
      ],
      "metadata": {
        "id": "pwviFRkJ9bZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def towardsdatascience(url):\n",
        "    # Make a GET request to the topic URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content of the topic page (excluding code snippets)\n",
        "    content = ''\n",
        "    for tag in soup.find_all('p',{'class':'pw-post-body-paragraph'}):\n",
        "        content += tag.text.strip() \n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "l5l2yGPUGv3I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# towardsdatascience(\"https://towardsdatascience.com/understanding-random-forest-58381e0602d2\")"
      ],
      "metadata": {
        "id": "54eE3FrfHd3-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JavaTPoint"
      ],
      "metadata": {
        "id": "YnUoA8D09n5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def javatpoint(url):\n",
        "    # Make a GET request to the topic URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content of the topic page (excluding code snippets)\n",
        "    content = ''\n",
        "    for tag in soup.find_all('div',{'class':'onlycontentinner'}):\n",
        "      for sub_tag in tag.find_all('p'):\n",
        "        content += sub_tag.text.strip() \n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "nnhjjIeKIcWQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# javatpoint(\"https://www.javatpoint.com/machine-learning-random-forest-algorithm\")"
      ],
      "metadata": {
        "id": "X-ixNcDeKjvP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TutorialsPoint"
      ],
      "metadata": {
        "id": "jEJ6rcES-D5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def tutorialspoint(url):\n",
        "      # Make a GET request to the topic URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content of the topic page (excluding code snippets)\n",
        "    content = ''\n",
        "    for tag in soup.find_all('div',{'id':'mainContent'}):\n",
        "        for sub_tag in tag.find_all('p'):\n",
        "          content += sub_tag.text.strip() \n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "iZazJXZH-JVA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tutorialspoint(\"https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_classification_algorithms_random_forest.htm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vBunOm0cLm9y",
        "outputId": "f4885eae-9c97-4e79-bd48-c4ed67338bf0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Random forest is a supervised learning algorithm which is used for both classification as well as regression. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which is better than a single decision tree because it reduces the over-fitting by averaging the result.We can understand the working of Random Forest algorithm with the help of following steps −Step 1 − First, start with the selection of random samples from a given dataset.Step 2 − Next, this algorithm will construct a decision tree for every sample. Then it will get the prediction result from every decision tree.Step 3 − In this step, voting will be performed for every predicted result.Step 4 − At last, select the most voted prediction result as the final prediction result.The following diagram will illustrate its working −First, start with importing necessary Python packages −Next, download the iris dataset from its weblink as follows −Next, we need to assign column names to the dataset as follows −Now, we need to read dataset to pandas dataframe as follows −Data Preprocessing will be done with the help of following script lines.Next, we will divide the data into train and test split. The following code will split the dataset into 70% training data and 30% of testing data −Next, train the model with the help of RandomForestClassifier class of sklearn as follows −At last, we need to make prediction. It can be done with the help of following script −Next, print the results as follows −The following are the advantages of Random Forest algorithm −It overcomes the problem of overfitting by averaging or combining the results of different decision trees.Random forests work well for a large range of data items than a single decision tree does.Random forest has less variance then single decision tree.Random forests are very flexible and possess very high accuracy.Scaling of data does not require in random forest algorithm. It maintains good accuracy even after providing data without scaling.Random Forest algorithms maintains good accuracy even a large proportion of the data is missing.The following are the disadvantages of Random Forest algorithm −Complexity is the main disadvantage of Random forest algorithms.Construction of Random forests are much harder and time-consuming than decision trees.More computational resources are required to implement Random Forest algorithm.It is less intuitive in case when we have a large collection of decision trees.The prediction process using random forests is very time-consuming in comparison with other algorithms.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyticsvidhya"
      ],
      "metadata": {
        "id": "f3z-LBqlSHJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def analyticsvidhya(url):\n",
        "      # Make a GET request to the topic URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content of the topic page (excluding code snippets)\n",
        "    content = ''\n",
        "    for tag in soup.find_all('section',{'class':'av-details-page'}):\n",
        "        for sub_tag in tag.find_all('p'):\n",
        "          content += sub_tag.text.strip() \n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "jCXOyLiaSGeg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyticsvidhya(\"https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "09Zu55nRSLU5",
        "outputId": "d38261f4-a468-4811-81ea-c92e53811a36"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Random Forest is one of the most popular and commonly used algorithms by Data Scientists. Random forest is a Supervised Machine Learning Algorithm\\xa0that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.One of the most important features of the Random Forest Algorithm is that it can handle the data set containing continuous variables, as in the case of regression, and categorical variables, as in the case of classification. It performs better for classification and regression tasks. In this tutorial, we will understand the working of random forest and implement random forest on a classification task.Learning ObjectivesThis article was published as a part of the Data Science Blogathon.Let’s dive into a real-life analogy to understand this concept further. A student named X wants to choose a course after his 10+2, and he is confused about the choice of course based on his skill set. So he decides to consult various people like his cousins, teachers, parents, degree students, and working people. He asks them varied questions like why he should choose, job opportunities with that course, course fee, etc. Finally, after consulting various people about the course he decides to take the course suggested by most people.Before understanding the working of the random forest algorithm in machine learning, we must look into the ensemble learning technique. Ensemble simplymeans combining multiple models. Thus a collection of models is used to make predictions rather than an individual model.Ensemble uses two types of methods:1. Bagging– It creates a different training subset from sample training data with replacement & the final output is based on majority voting. For example,\\xa0 Random Forest.2. Boosting– It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example,\\xa0 ADA BOOST, XG BOOST.As mentioned earlier, Random forest works on the Bagging principle. Now let’s\\xa0dive in and understand bagging in detail.Bagging, also known as\\xa0Bootstrap Aggregation, is the ensemble technique used by random forest.Bagging chooses a random sample/random subset from the entire data set. Hence each model is generated from the samples (Bootstrap Samples) provided by the Original Data with replacement known as row sampling. This step of row sampling with replacement is called bootstrap. Now each model is trained independently, which generates results. The final output is based on majority voting after combining the results of all models. This step which involves combining all the results and generating output based on majority voting, is known as aggregation.Now let’s look at an example by breaking it down with the help of the following figure. Here the bootstrap sample is taken from actual data (Bootstrap sample 01, Bootstrap sample 02, and Bootstrap sample 03) with a replacement which means there is a high possibility that each sample won’t contain unique data. The model (Model 01, Model 02, and Model 03) obtained from this bootstrap sample is trained independently. Each model generates results as shown. Now the Happy emoji has a majority when compared to the Sad emoji. Thus based on majority voting final output is obtained as Happy emoji.Boosting is one of the techniques that use the concept of ensemble learning. A boosting algorithm combines multiple simple models (also known as weak learners or base estimators) to generate the final output. It is done by building a model by using weak models in series.There are several boosting algorithms; AdaBoost was the first really successful boosting algorithm that was developed for the purpose of binary classification. AdaBoost is an abbreviation for Adaptive Boosting and is a prevalent boosting technique that combines multiple “weak classifiers” into a single “strong classifier.” There are Other Boosting techniques. For more, you can visit4 Boosting Algorithms You Should Know – GBM, XGBoost, LightGBM & CatBoostStep 1: In the Random forest model, a subset of data points and a subset of features is selected for constructing each decision tree. Simply put, n random records and m features are taken from the data set having k number of records.Step 2: Individual decision trees are constructed for each sample.Step 3: Each decision tree will generate an output.Step 4: Final output is considered based on Majority Voting or Averaging for Classification and regression, respectively.For example:\\xa0 consider the fruit basket as the data as shown in the figure below. Now n number of samples are taken from the fruit basket, and an individual decision tree is constructed for each sample. Each decision tree will generate an output, as shown in the figure. The final output is considered based on majority voting. In the below figure, you can see that the majority decision tree gives output as an apple when compared to a banana, so the final output is taken as an apple.Random forest is a collection of decision trees; still, there are a lot of differences in their behavior.Thus random forests are much more successful than decision trees only if the trees are diverse and acceptable.Hyperparameters are used in random forests to either enhance the performance and predictive power of models or to make the model faster.n_estimators: Number of trees the algorithm builds before averaging the predictions.max_features: Maximum number of features random forest considers splitting a node.mini_sample_leaf: Determines the minimum number of leaves required to split an internal node.criterion: How to split the node in each tree? (Entropy/Gini impurity/Log Loss)max_leaf_nodes: Maximum leaf nodes in each treen_jobs: it tells the engine how many processors it is allowed to use. If the value is 1, it can use only one processor, but if the value is -1, there is no limit.random_state:controls randomness of the sample. The model will always produce the same results if it has a definite value of random state and has been given the same hyperparameters and training data.oob_score:\\xa0OOB means out of the bag. It is a random forest cross-validation method. In this, one-third of the sample is not used to train the data; instead used to evaluate its performance. These samples are called out-of-bag samples.Now let’s implement Random Forest in scikit-learn.Python Code:From hyperparameter tuning, we can fetch the best estimator, as shown. The best set of parameters identified was max_depth=5, min_samples_leaf=10,n_estimators=10The trees created by estimators_[5] and estimators_[7] are different. Thus we can say that each tree is independent of the other.This algorithm is widely used in E-commerce, banking, medicine, the stock market, etc.For example: In the Banking industry, it can be used to find which customer will default on a loan.1. It can be used in classification and regression problems.2. It solves the problem of overfitting\\xa0as output is based on majority voting or averaging.3. It performs well even if the data contains null/missing values.4. Each decision tree created is independent of the other; thus, it shows the property of parallelization.5. It is highly stable as the average answers given by a large number of trees are taken.6. It maintains diversity as all the attributes are not considered while making each decision tree though it is not true in all cases.7. It is immune to the curse of dimensionality. Since each tree does not consider all the attributes, feature space is reduced.8. We don’t have to segregate data into train and test as there will always be 30% of the data, which is not seen by the decision tree made out of bootstrap.1. Random forest is highly complex compared to decision trees, where decisions can be made by following the path of the tree.2. Training time is more than other models due to its complexity. Whenever it has to make a prediction, each decision tree has to generate output for the given input data.Random forest is a great choice if anyone wants to build the model fast and efficiently, as one of the best things about the random forest is it can handle missing values. It is one of the best techniques with high performance, widely used in various industries for its efficiency. It can handle binary, continuous, and categorical data. Overall, random forest is a fast, simple, flexible, and robust model with some limitations.Key TakeawaysA. Random Forest is a supervised learning algorithm that works on the concept of bagging. In bagging, a group of models is trained on different subsets of the dataset, and the final output is generated by collating the outputs of all the different models. In the case of random forest, the base model is a decision tree.A. The following steps will tell you how random forest works:A. Random Forest tends to have a low bias since it works on the concept of bagging. It works well even with a dataset with a large no. of features since it works on a subset of features. Moreover, it is faster to train as the trees are independent of each other, making the training process parallelizable.The media shown in this article are not owned by Analytics Vidhya and are used at the Author’s discretion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General scrapper"
      ],
      "metadata": {
        "id": "sFUFTT4_-GwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def general_scrapper(url):\n",
        "      # Make a GET request to the topic URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content of the topic page (excluding code snippets)\n",
        "    content = ''\n",
        "    for tag in soup.find_all('p'):\n",
        "        content += tag.text.strip() \n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "iuDhQAxB-OCd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# general_scrapper(\"https://www.ibm.com/in-en/topics/random-forest\")"
      ],
      "metadata": {
        "id": "mxEuN5U2MHI6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrapper\n",
        "Identify the domain of website and call apprpriate scrapping function  \n",
        "Call general scrapper if not in any *domain*"
      ],
      "metadata": {
        "id": "DkWY4d_z3aRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tld"
      ],
      "metadata": {
        "id": "AJ5w6iG54_sA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfb4a17-e050-422c-dc22-de6913a18287"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tld\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tld\n",
            "Successfully installed tld-0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tld import get_tld\n",
        "\n",
        "data_list = []\n",
        "\n",
        "# go through each url\n",
        "for url in url_list:\n",
        "\n",
        "  # Get the domain from url\n",
        "  res = get_tld(url, as_object=True)\n",
        "  current_domain = res.domain\n",
        "\n",
        "  if current_domain == \"wikipedia\":\n",
        "    data_list.append(wikipedia(url))\n",
        "  elif current_domain == \"geekforgeeks\":\n",
        "    data_list.append(geeksforgeeks(url))\n",
        "  elif current_domain == \"towardsdatascience\" or current_domain == \"medium\":\n",
        "    data_list.append(towardsdatascience(url))\n",
        "  elif current_domain == \"w3schools\":\n",
        "    data_list.append(w3schools(url))\n",
        "  elif current_domain == \"tutorialspoint\":\n",
        "    data_list.append(tutorialspoint(url))\n",
        "  elif current_domain == \"analyticsvidhya\":\n",
        "    data_list.append(analyticsvidhya(url))\n",
        "  elif current_domain == \"javatpoint\":\n",
        "    data_list.append(javatpoint(url))\n",
        "  else:\n",
        "    data_list.append(general_scrapper(url))\n"
      ],
      "metadata": {
        "id": "gXgk6GS32nn1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Data Cleaning\n",
        "* Prepare data to be fed into text summarizer\n",
        "* Remove HTML tags, special characters, punctuation\n",
        "* Perform \n",
        "  * Spellcheck\n",
        "  * Normalization"
      ],
      "metadata": {
        "id": "vuGB9jnOMMHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "CLEANR = re.compile('<.*?>') \n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleantext = re.sub(CLEANR, '', raw_html)\n",
        "  return cleantext"
      ],
      "metadata": {
        "id": "l5e5Uc25UYZc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data_list)):\n",
        "  # Remoove HTML\n",
        "  data_list[i] = cleanhtml(data_list[i])\n",
        "  # Remove special charcters\n",
        "  data_list[i] = re.sub(r'[^a-zA-Z\\s\\.]+', ' ', data_list[i])\n",
        "  # Remove newline\n",
        "  data_list[i] = re.sub('\\n', ' ', data_list[i])\n",
        "  # Remove extra spaces\n",
        "  # data_list[i] = \" \".join(data_list[i].split())"
      ],
      "metadata": {
        "id": "lyyg-AzeMTMo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SpellCheck\n",
        "# data_list_new = []\n",
        "# from textblob import TextBlob\n",
        "# for i in range(len(data_list)):\n",
        "#   data_list_new.append(TextBlob(data_list[i]).correct())"
      ],
      "metadata": {
        "id": "UJATrsAFMTQQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Normalization"
      ],
      "metadata": {
        "id": "xdPC0SKEyO07"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining everything into a single text\n",
        "\n",
        "text = \" \".join(data_list)"
      ],
      "metadata": {
        "id": "fH3YZQfVyLNg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Text summarization\n",
        "* Summarize data into a single essay\n",
        "* Abstarctive Summarization"
      ],
      "metadata": {
        "id": "trumiY77MTzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Transformers library\n",
        "!pip install datsets transformers[sentencepiece]\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0SXqQYs6wt-",
        "outputId": "b8725985-3272-4507-c136-188fe501a0b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datsets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for datsets\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import transformers\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Define the input text and the summary length\n",
        "max_length = 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjNJeY8O51vx",
        "outputId": "76e59eab-48bd-4d0e-98c8-c5eeac07466d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text and encode it as input for the model\n",
        "input_text = \"summarize: \" + text\n",
        "input_ids = tokenizer.encode(input_text, truncation=True, max_length=512,return_tensors='pt')\n",
        "\n",
        "# Generate a summary\n",
        "summary = model.generate(input_ids, min_length = 500, max_length=max_length)"
      ],
      "metadata": {
        "id": "AFA3oBCq66Wm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the summary\n",
        "summary_text = tokenizer.decode(summary[0], skip_special_tokens=True)\n",
        "print(summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eYfh3lz69QP",
        "outputId": "8f09cfad-c641-4d9a-bca5-474389057797"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the top down approach is a good framework to adopt when learning technical concepts. it makes debugging the algorithms much easier so we can better interpret the outcome of a model. Jeremy Howard and Rachel Thomas are the founders of fast.ai. they constantly advocate for this approach. he says that if you have never implemented a Random forest before that you do so and make some alterations to the hyperparameters parameters set before training. he says that waking through the notebook is really useful even............................. the. the.. the. the.................................................................................................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Document generation\n",
        "* generate a docx file with title, content and references"
      ],
      "metadata": {
        "id": "2Ehg28zwMiUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciEdO6EtLh-7",
        "outputId": "b2998569-acbe-4933-814d-2f62719bf65b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/5.6 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m5.4/5.6 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=330fbd70fd77e9fe99e2242df5bb2eb15e4b8ff73ed2b21c7824c797ca35137e\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/8b/7c/09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new Word document\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "document = Document()"
      ],
      "metadata": {
        "id": "v8I94OdSMtXD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add the summary to the Word document\n",
        "document.add_heading(search_term,0)\n",
        "document.add_paragraph(summary_text)"
      ],
      "metadata": {
        "id": "-f7NppxNcnhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3934fe3f-c1e5-42b1-a38e-34d250e93e3a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<docx.text.paragraph.Paragraph at 0x7f91b918f520>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add the references to the Word document\n",
        "document.add_heading(\"References\",level = 2)\n",
        "count = 1\n",
        "for url in url_list:\n",
        "  temp_text =  str(count) + \". \" + url\n",
        "  document.add_paragraph(temp_text)\n",
        "  count+=1"
      ],
      "metadata": {
        "id": "wPjvX5nxMtJI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the Word document\n",
        "# save the Word document\n",
        "document.save(\"{}.docx\".format(search_term))"
      ],
      "metadata": {
        "id": "WjYK7coSMtDR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. https://chat.openai.com/chat"
      ],
      "metadata": {
        "id": "gK3jP5YTMuG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Mining Project - Review 1\n"
      ],
      "metadata": {
        "id": "aJ_l0GEqUFte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Steps:\n",
        "1. We define standard websites from which we will scrap the required data.\\n   \n",
        "These are treated as trusted and verified sources\n",
        "2. Scrap data from more websites.\n",
        "3. Information Extraction algorithm extracts data from these sources\n",
        "4. Use Natural language processing for text summarisation.\n",
        "5. Generate a word file with these outputs and references included.\n"
      ],
      "metadata": {
        "id": "xKR2PdLAy2tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define sources"
      ],
      "metadata": {
        "id": "wbFzgAuqUb32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJATZv47S1X7"
      },
      "outputs": [],
      "source": [
        "# define the search keyword\n",
        "keyword = \"technology\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the sources to search from\n",
        "# trusted sources\n",
        "sources = [\n",
        "    {\n",
        "        \"name\": \"BBC News\",\n",
        "        \"url\": \"https://www.bbc.com/news/technology\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"CNN\",\n",
        "        \"url\": \"https://www.cnn.com/search/?q=technology&size=10&type=article\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"The Verge\",\n",
        "        \"url\": \"https://www.theverge.com/search?q=technology&sort=relevance\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "8gyGui4mUfWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information retrieval"
      ],
      "metadata": {
        "id": "sdVfkGhdUf_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# define empty lists to store the article titles and summaries\n",
        "titles = []\n",
        "summaries = []\n",
        "\n",
        "# loop through each source and scrape the articles\n",
        "for source in sources:\n",
        "    response = requests.get(source[\"url\"])\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    \n",
        "    # find all the articles that contain the search keyword\n",
        "    articles = soup.find_all(\"article\", {\"class\": \"css-13mho3u\"})\n",
        "    for article in articles:\n",
        "        if keyword.lower() in article.text.lower():\n",
        "            title = article.find(\"h3\", {\"class\": \"gs-c-promo-heading__title\"}).text.strip()\n",
        "            summary = article.find(\"p\", {\"class\": \"gs-c-promo-summary\"}).text.strip()\n",
        "            titles.append(title)\n",
        "            summaries.append(summary)\n",
        "\n",
        "# print the results\n",
        "print(\"Titles:\")\n",
        "print(titles)\n",
        "print(\"Summaries:\")\n",
        "print(summaries)\n"
      ],
      "metadata": {
        "id": "Gu76P0C7UuRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31109725-40de-4d90-bfc5-f3b5b0f0c8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titles:\n",
            "[]\n",
            "Summaries:\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__sv46vLUukD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information Extraction"
      ],
      "metadata": {
        "id": "tlptPF06Uu6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# load the pre-trained model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# define the list of summaries\n",
        "summaries = [\n",
        "    \"Technology giant Apple has unveiled a new range of products at its annual event in California.\",\n",
        "    \"Amazon is set to launch a new service that will allow customers to try on clothes virtually.\",\n",
        "    \"Microsoft has announced plans to acquire a leading AI research company for $19.7 billion.\",\n",
        "    \"Tesla's new electric vehicle has received over 500,000 pre-orders, according to the company.\",\n",
        "    \"Google has teamed up with a major pharmaceutical company to develop new treatments for diseases.\"\n",
        "]\n",
        "\n",
        "# define empty lists to store the named entities\n",
        "people = []\n",
        "organizations = []\n",
        "locations = []\n",
        "\n",
        "# loop through each summary and extract the named entities\n",
        "for summary in summaries:\n",
        "    doc = nlp(summary)\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == \"PERSON\":\n",
        "            people.append(entity.text)\n",
        "        elif entity.label_ == \"ORG\":\n",
        "            organizations.append(entity.text)\n",
        "        elif entity.label_ == \"GPE\":\n",
        "            locations.append(entity.text)\n",
        "\n",
        "# print the results\n",
        "print(\"People:\")\n",
        "print(people)\n",
        "print(\"Organizations:\")\n",
        "print(organizations)\n",
        "print(\"Locations:\")\n",
        "print(locations)\n"
      ],
      "metadata": {
        "id": "9jbMj-7PUyGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f8a236-819a-45c8-84a3-837a5f215df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "People:\n",
            "[]\n",
            "Organizations:\n",
            "['Apple', 'Amazon', 'Microsoft', 'AI', 'Tesla', 'Google']\n",
            "Locations:\n",
            "['California']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kU6-zkObUxpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text summarization"
      ],
      "metadata": {
        "id": "Kjohlf0YUyeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.summarization.summarizer import summarize\n",
        "\n",
        "# define the list of summaries generated in the previous question\n",
        "summaries = [\n",
        "    \"Technology giant Apple has unveiled a new range of products at its annual event in California.\",\n",
        "    \"Amazon is set to launch a new service that will allow customers to try on clothes virtually.\",\n",
        "    \"Microsoft has announced plans to acquire a leading AI research company for $19.7 billion.\",\n",
        "    \"Tesla's new electric vehicle has received over 500,000 pre-orders, according to the company.\",\n",
        "    \"Google has teamed up with a major pharmaceutical company to develop new treatments for diseases.\"\n",
        "]\n",
        "\n",
        "# combine the summaries into a single string\n",
        "text = \" \".join(summaries)\n",
        "\n",
        "# generate the summary using the TextRank algorithm\n",
        "summary = summarize(text)\n",
        "\n",
        "# print the results\n",
        "print(\"Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "NInf6Vc1U439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867eb546-6fda-476a-9eec-ec4ada33f7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.summarization.summarizer:Input text is expected to have at least 10 sentences.\n",
            "WARNING:gensim.summarization.summarizer:Input corpus is expected to have at least 10 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "Tesla's new electric vehicle has received over 500,000 pre-orders, according to the company.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMPPNV-NU4v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output generation"
      ],
      "metadata": {
        "id": "rqvAU_VrU4Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SBqmQWflU7y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBFcZ6RaU7ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4vlp3QvtVhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combined"
      ],
      "metadata": {
        "id": "akHM17BvtWKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we define a list of trusted websites to scrape data from and use requests and BeautifulSoup to extract headlines from the HTML content of these websites. We then ask the user for additional websites to scrape data from and use spaCy to perform information extraction on the paragraphs of text from these websites. We combine all the extracted data into a single string and generate a summary using the TextRank algorithm from gensim. Finally, we create a new Word document using the python-docx library, add the summary and references to the document, and save it as a file named \"output.docx\". Note that you can modify the code to scrape different types of data or adjust the parameters of the information extraction and summarization algorithms based on your specific requirements."
      ],
      "metadata": {
        "id": "n6Du9qWytcwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwN3YSxttkO1",
        "outputId": "186cd031-987b-4b0d-f5af-5428947f7efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=899f9e2641aa6cc90c03f6bcf9b0dc65df03854576d1c50dd41869d90f9638bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/8b/7c/09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim.summarization.summarizer import summarize\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "\n",
        "# define the list of trusted websites to scrape data from\n",
        "trusted_websites = [\n",
        "    \"https://www.bbc.com/news\",\n",
        "    \"https://www.nytimes.com/\",\n",
        "    \"https://www.theguardian.com/international\"\n",
        "]\n",
        "\n",
        "# define empty lists to store the extracted data\n",
        "summaries = []\n",
        "people = []\n",
        "organizations = []\n",
        "locations = []\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# scrape data from trusted websites\n",
        "for website in trusted_websites:\n",
        "    response = requests.get(website)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    headlines = soup.find_all(\"h3\")\n",
        "    for headline in headlines:\n",
        "        summaries.append(headline.text.strip())\n",
        "\n",
        "# ask user for additional websites to scrape data from\n",
        "additional_websites = input(\"Enter additional websites to scrape data from (separated by commas): \").split(\",\")\n",
        "\n",
        "# scrape data from additional websites\n",
        "for website in additional_websites:\n",
        "    response = requests.get(website.strip())\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    for paragraph in paragraphs:\n",
        "        doc = nlp(paragraph.text)\n",
        "        for entity in doc.ents:\n",
        "            if entity.label_ == \"PERSON\":\n",
        "                people.append(entity.text)\n",
        "            elif entity.label_ == \"ORG\":\n",
        "                organizations.append(entity.text)\n",
        "            elif entity.label_ == \"GPE\":\n",
        "                locations.append(entity.text)\n",
        "\n",
        "# combine the extracted data into a single string\n",
        "text = \" \".join(summaries + people + organizations + locations)\n",
        "\n",
        "# generate the summary using the TextRank algorithm\n",
        "summary = summarize(text)\n",
        "\n",
        "# create a new Word document\n",
        "document = Document()\n",
        "\n",
        "# add the summary to the Word document\n",
        "document.add_heading(\"Summary\")\n",
        "document.add_paragraph(summary)\n",
        "\n",
        "# add the references to the Word document\n",
        "document.add_heading(\"References\")\n",
        "for website in trusted_websites + additional_websites:\n",
        "    document.add_paragraph(website)\n",
        "\n",
        "# save the Word document\n",
        "document.save(\"output.docx\")\n"
      ],
      "metadata": {
        "id": "KE3umyZ7tXT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e35401-53de-4ca5-d9fd-1281809c9530"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter additional websites to scrape data from (separated by commas): https://www.wikipedia.org/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. https://www.tibco.com/reference-center/what-is-text-mining\n",
        "2. “Web Scraping with Python, Second\n",
        "Edition by Ryan Mitchell (O’Reilly). Copyright 2018 Ryan Mitchell,\n",
        "978-1-491-998557-1.”\n"
      ],
      "metadata": {
        "id": "wAO1l4UWU8Y5"
      }
    }
  ]
}